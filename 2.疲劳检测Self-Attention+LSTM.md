---
headingNumber: true
enableMacro: true
customVar: Hello
define:
    --Author--: ProtoDrive000
---
# 疲劳检测Self-Attention+LSTM


| 论文名称 | LSTM Model with Self-Attention Mechanism for EEG Based Cross-Subject Fatigue Detection |
| -- | -- | 
| 期刊 | 2021 IEEE 3rd International Conference on Frontiers Technology of Information and Computer (ICFTIC) |
| 方法 | 本文基于基于自我注意的长短时记忆（LSTM）模型，提出了一种新的高精度深度学习模型。我们的研究表明，LSTM可以找到采集通道之间每个频带的相关特征，而不是将高维EEG数据独立串联成特征向量；自我注意机制可以从高维数据中选择对当前任务更关键的信息。在实验中，我们选择的公共数据集被标记为两个疲劳水平，通过随机删除大多数样本来实现样本平衡。 |
| 结论 | 模型实现了78.84%的准确率，并且在跨主题情况下优于其他疲劳检测方法。具体而言，基于自我注意的LSTM比EEGNet提高了19.84%的准确率，并使受试者匹配提高了4.52%。 |

在本文中，我们提出了一种新的疲劳检测深度学习模型。该模型利用脑电信号的功率谱密度特征作为输入，然后使用基于自我注意的LSTM来计算疲劳状态的概率。

## 数据集描述
在整个90分钟的实验中，使用30个EEG电极和2个参考电极记录EEG数据。EEG电极放置在改进的10-20系统中，电极与皮肤之间的接触阻抗小于5KΩ。
## 数据处理
实验中，我们将原始EEG数据下采样到128Hz。然后，我们在车道偏离事件发生前3秒截获了EEG信号，并将车道偏离事件后的反应时间（RT）用作疲劳指标。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815030124.png#pic_center%20=400x)


此外，对每个受试者的EEG数据按空间和时间顺序进行滤波。表面拉普拉斯算子用于空间滤波。表面拉普拉斯空间滤波通过滤除几个相邻电极之间的空间特征来提高EEG信号的空间分辨率。具体实现如公式所示。
$$ V_{L A P(i)}=V_{(i)}-\sum_{j \in S(i)} w_{i, j} V_{(j)} $$
$$ w_{i, j}=\frac{1 / d_{i, j}}{\sum_{j \in S(i)} 1 / d_{i, j}} $$
在时域中，使用频率范围为1HZ到30Hz的==6阶巴特沃斯带通滤波器==对EEG信号的每个通道进行滤波。最后，我们得到的每个样本是一个具有相应RT的30×384矩阵。

| 名称 | 描述|
| -- | -- | 
|局部RT|与单车道偏离事件相对应的RT|
|全局RT|每个车道偏离事件前90秒内所有时段的局部RT平均值|
|警报RT|对实验中的所有局部RT进行排序,局部RT的第5位|
|唤醒状态|局部RT和全局RT小于1.5倍警报RT的事件|
|疲劳状态|大于2.5倍警告RT的事件记录|

## 特征提取
EEG信号的频域特征是突出的。对每个事件的EEG数据应用带Hamming窗的短时傅立叶变换（STFT）,最后，从4个频带获得的特征数据是具有相应RT的30×4矩阵。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815030329.png#pic_center%20=400x)

## LSTM
### 简介
长短时记忆网络（LSTM）是一种特殊的递归神经网络（RNN）；它非常擅长处理序列数据。EEG信号可以看作是时间序列和空间序列。在时域，EEG数据类似于时变声波。在空间上，由于其多通道特性，不同的EEG通道传输特定和相关的信息。例如，前额和后颈的脑电图信号通常表达不同的含义。因此，利用LSTM模型提取深层特征是一种可行的方法。
### 组成
LSTM网络由==细胞==组成，细胞的输出根据过去的记忆内容通过网络演进。

这些细胞有一个共同的细胞状态，在整个细胞LSTM链上保持长期依赖关系。然后，信息流由==输入门==(it)和==忘记门==(ft)控制，从而允许网络决定是忘记之前的状态$C_{t-1}$还是用新信息更新当前状态$C_{t}$。每个单元的输出(隐藏状态)由一个==输出门==$o_{t}$控制，允许单元计算其输出给定更新的单元状态.

![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220808030020.png#pic_center%20=400x)

描述LSTM单元结构的公式如下:
$$\mathbf{i}_{\mathbf{t}}=\sigma\left(\mathbf{W}_{\mathbf{i}} \cdot\left[\mathbf{h}_{\mathbf{t}-\mathbf{1}}, \mathbf{x}_{\mathbf{t}}\right]+\mathbf{b}_{\mathbf{i}}\right)$$
$$\mathbf{f}_{\mathbf{t}}=\sigma\left(\mathbf{W}_{\mathbf{f}} \cdot\left[\mathbf{h}_{\mathbf{t}-\mathbf{1}}, \mathbf{x}_{\mathbf{t}}\right]+\mathbf{b}_{\mathbf{f}}\right)$$
$$\mathbf{C}_{\mathbf{t}}=\mathbf{f}_{\mathbf{t}} * \mathbf{C}_{\mathbf{t}-\mathbf{1}}+\mathbf{i}_{\mathbf{t}} * \tanh \left(\mathbf{W}_{\mathbf{c}} \cdot\left[\mathbf{h}_{\mathbf{t}-\mathbf{1}}, \mathbf{x}_{\mathbf{t}}\right]+\mathbf{b}_{\mathbf{c}}\right)$$
$$\mathbf{o}_{\mathbf{t}}=\sigma\left(\mathbf{W}_{\mathbf{o}} \cdot\left[\mathbf{h}_{\mathbf{t}-\mathbf{1}}, \mathbf{x}_{\mathbf{t}}\right]+\mathbf{b}_{\mathbf{o}}\right)$$
$$\mathbf{h}_{\mathbf{t}}=\mathbf{o}_{\mathbf{t}} * \tanh \left(\mathbf{C}_{\mathbf{t}}\right)$$


$$ \sigma(x)=\frac{1}{1+e^{-x}}, \tanh (x)=\frac{2}{1+e^{-2 x}}-1 $$

| 名称 | 描述|
| -- | -- | 
|$h_t$|是时间步长t时的隐藏状态|
|$C_{t-1}$|是时间步长t时的细胞状态|
|$x_t$|是馈入细胞的输入特征|
|$W_f$, $W_i$, $W_c$, $W_o$|是权重|
|$b_f$, $b_i$, $b_c$, $b_o$|是通过时间反向传播可以得到的偏差|


### 参数设置
LSTM的超参数会影响模型的分类性能，我们没有选择更深的LSTM层的原因是为了防止过度拟合。

| 名称 | 描述|
| -- | -- | 
|隐藏层大小|4|
|LSTM单元的dropout|50%|
|LSTM层的数量|1|

## 自我注意力机制
自我注意机制使用注意机制动态”生成不同连接的权重，以处理可变长度信息序列。在我们的方法中，自我注意机制的输入是LSTM模型的输出。我们将输入向量定义为：X
然后将其作为query向量Q,key向量K,value向量V
$$ Q=W_{Q} X $$
$$ K=W_{K} X $$
$$ V=W_{V} X $$
根据上述公式，Q在自我注意机制中，是输入X的转换 而在传统的注意机制中，Q来自外部。
自我注意机制使用==点积==模型，输出向量可以写成公式。
$$ H=V \operatorname{softmax}\left(\frac{K^{\mathrm{T}} Q}{\sqrt{d_{k}}}\right) $$
之后$d_k$按比例缩放时，通过softmax函数获得每个输出通道上的注意力分数。
自我注意机制中的参数为：==K = V = X==, 以及值向量V是LSTM模型50%dropout的输出 。


最后，我们传递输出向量H通过具有sigmoid激活功能的全连接层
$$S(x)=1/(1+e^{-x})$$
sigmoid的输出值在0-1之间，这意味着当前样本属于==疲劳状态==或==唤醒状态==的概率。
### 错误
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815032422.png#pic_center =400x)

## 迁移学习

步骤1.我们在源数据集（训练主题）上预训练源模型。

步骤2.创建一个新模型作为目标模型。它在源模型（输出层除外）上复制所有模型设计及其参数。我们假设这些模型参数包含从源训练对象学习的知识，这也适用于测试对象。我们还假设源模型的输出层与训练对象的标签密切相关，因此不在目标模型中使用。

步骤3.随机初始化输出层的模型参数，并为隐藏层设置最小学习速率（微调）

步骤4.在训练主题上训练目标模型。我们将重新训练输出层，其他层的参数将根据源模型的参数进行微调。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815032953.png#pic_center%20=400x)

## 结果
在实验中，我们使用剔除一个受试者的交叉验证来测试模型的分类精度。在相同的实验条件下，我们比较了基于注意力的LSTM模型与EEGNet、SVM和主题匹配的分类精度。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815033138.png#pic_center%20=400x)

![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815033113.png#pic_center%20=400x)

基于注意力的LSTM模型的分类精度最高，为78.84%。与同样使用深度学习模型的EEGNet相比，我们的模型将分类准确率提高了19.84%。与传统的机器学习方法SVM相比，基于注意力的LSTM模型的分类精度提高了7.17%。与主题匹配相比，我们的模型将准确率提高了4.52%。此外，我们的模型分类精度的标准差（STD）也很突出。我们的方法的标准偏差为6.53，低于主题匹配和支持向量机。虽然标准偏差不如EEGNet好，但它在精度上有显著优势。结果表明，该模型在跨被试情境下具有==较好的泛化能力==。

正如我们所期望的，通过使用基于注意力的LSTM模型，我们取得了良好的结果。然而，我们认为深度学习模型的EEGNet没有取得好的结果，因为EEGNet广泛地学习每个通道的PSD特征，而没有整合每个通道的信息。

在我们提出的模型中，LSTM模型将每个通道之间的PSD特征关联起来，注意机制评估隐藏在每个LSTM单元中的特征的重要性。并非每个特征对于高维数据都具有相同的重要性，手动选择关键特征具有挑战性，我们的模型解决了这个问题。

## 补充
### Laplacian方法
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815025146.png#pic_center%20=400x)
