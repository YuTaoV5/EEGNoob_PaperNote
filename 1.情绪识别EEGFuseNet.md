---
headingNumber: true
enableMacro: true
customVar: Hello
define:
    --Author--: ProtoDrive000
    --te--: ==transformer encoders==
    --sa--: ==self-attention==
---
# 1.情绪识别EEGFuseNet

| 论文名称 |EEG classification algorithm of motor imagery based on CNN-Transformer fusion network |
| -------- | -------- | 
| 期刊 |IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILIT A TION ENGINEERING 4.528/Q1|
| 方法 |本文提出了一种实用的基于混合无监督深度卷积递归生成对抗网络的脑电特征表征与融合模型，称为EEGFuseNet。EEGFuseNet以无监督的方式进行训练，覆盖空间和时间动态的深度脑电图特征被自动识别。|
| 结论 |证明了以纯无监督的方式实现基于脑电的跨主体情绪识别的可能性。|
| 评价 |EEGFuseNet作为一个无监督模型，具有良好的跨主体情感识别性能。这项工作可以作为无监督EEG分类的一种奠基的方法，但由于缺乏标签指导，当前无监督结果的性能仍然低于有监督方法。|

---
## 主要工作


### EEGFuseNet
提出了一种高效的**深度编码器-解码器混合网络结构**来表征非平稳时间序列脑电信号。有效地**融合了时空动力学的联合信息，刻画了有用但潜在的时空动力学信息**。该混合网络通过融合CNN、递归神经网络(RNN)、生成对抗网络(GAN)

以一种智能混合的方式融合不同来源的特征信息。具体而言，
- 在`CNN`从原始脑电图信号中提取特征的基础上
- 采用`RNN`通过探索时间邻接处潜在的特征关系来增强特征表示
- 为了提高训练性能，引入`GAN`以无监督的方式通过动态更新来改进`CNN-RNN`网络的训练过程

这可能有利于高质量的特征生成。提取的深度特征可以表征通道间的空间关系和相邻时间点信号的依赖关系。
### 超图解码模型
[什么是超图](#什么是超图)

建立了统一的基于无监督`EEGFuseNet`的==超图解码框架==，并验证了该框架在解决脑解码应用中的可行性和有效性。
本文根据超图构造中的**特征深度特征**来衡量样本关系，其中**共享相似属性**的脑电图样本由超边缘连接

::: tip 为啥用超边缘连接
超边缘可以更灵活地描述群体关系。
:::
遵循超图划分规则，然后对**超图的拉普拉斯式**进行计算和优化.
- 其中共享相似属性的超边之间的连接被分组到一个集群中
- 而共享不同属性的超边之间的连接被分组到不同的集群中。

---
## 设计初衷
### 如何有效地融合在不同大脑位置和不同时间点采集的脑电图信号
::: tip 空间信息融合
基于**给定的时间点**，融合**不同脑区**关系层面的脑电图信号(如相关性)。通过
- 评估皮层区域之间的相互作用功能
- 测量相互作用的方向和强度
来解释皮层区域的交流行为。
:::
::: tip 时间信息融合
在**特定的大脑区域或脑电电极**上，通过计算**时域特征**(如统计模式和形状信息)、**频域特征**(如功率谱特征)或时频特征
- 将连续的时间序列脑电图信号截断为短数据段
- 融合不同时间点的脑电图样本。

两个著名的例子是==短时傅里叶变换==(STFT)和==小波分析==，它们都是通过逐次计算和测量数据在时间上的相互作用来计算数据的时频动态。这种融合适用于在特定电极上的时间、频率或时频域的常用脑电图特征。
:::
::: tip 空间和时间信息的融合
目前，许多研究尝试采用时序方法来表征时间信息和空间信息，
- 从每个电极上的脑电图数据来表征时间信息
- 根据每个电极上表征的时间信息来衡量任意两个电极之间或一组电极之间的关系来表征空间信息

虽然可以估计空间特征和时间特征，但这种方法受限于**预先定义的时空信息顺序关系**，不能有效地**从时空联合域中提取和融合有用但潜在**的信息。
:::
### 无监督学习
::: tip 为什么选择无监督学习作为研究方向
目前大部分研究都是基于监督学习的，这就需要**大量的带有情感标签的训练样本**，尤其是对于深度网络。例如，**训练样本的规模越小，深度网络就会因为过拟合问题而不能很好地泛化**。网络设计和样本量都会对网络性能产生显著影响。
- 在现实的应用场景中，从不同的参与者那里收集大量的脑电图信号，并手动为每个样本标注情绪标签是**不现实**的。
- 在示例注释过程中还存在诱发“**标签噪声**”的风险。

无监督学习可以提供一种更自然的方法来解码脑电图信号，并与人类学习机制更一致，需要从可用的样本中获得有用的信息，而不需要任何相关的指导。
:::
结果表明，所提出的无监督框架具有良好的泛化性，较好地解决了跨学科任务中的个体差异问题。无论现有的深度无监督脑电网络是怎样的，据我们所知，目前还没有对混合深度构型以融合无监督的方式将高维脑电信号转换为低维有效可靠的特征表示进行过扎实而深入的研究。

---
## EEGFuseNet具体思路

(1)如何从经典的`CNN`结构提出`EEGFuseNet`的基本架构，
(2)如何将`GAN`融合到基于`CNN`的网络中以生成高质量的特征，
(3)如何将`RNN`融合到基于`CNN-GAN`的网络中以更好地融合时空信息
## CNN
### 编码器-解码器架构
在典型的基于CNN的深度`编码器-解码器`网络中, 编码器进行卷积和下采样，解码器进行反卷积和上采样，重构输入EEG信号。
- 编码器由**卷积层**组成，用于从给定的输入X中提取有用的信息，并将其转换为单维向量(隐藏向量)
- 解码器由**反卷积层**组成，用于对编码器特征图进行拓展，并将隐藏向量转换为生成的输出(Y)。
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220912172436.png)
- 通过**最大化X和Y之间的相似性** 等价于 **最小化给定的均方误差MSE损失函数**$L ( X , Y ) = | | X - Y | | _ { 2 } ^ { 2 }$，联合训练自编码器结构，并将学习到的隐藏向量作为X的信息化潜在特征表示，用于进一步的数据分析和建模。
::: tip 优势
值得注意的是，该自编码器架构是一种**自学习范式**，在训练过程中**不需要任何标记信息**，与其他常见的特征提取架构相比，训练起来明显**更容易**。因此，它适用于解决标签缺失的小体积脑电数据的问题。
:::
### 编码器设计思路

![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220912145856.png)
#### 卷积层
受`EEGNet`结构的启发，利用连续的**二维卷积层**生成覆盖不同频段脑电图空间信息的特征映射，其中**卷积核长度为输入数据采样率的一半**。值得注意的是，由于输入的EEG信号由通道和时间点组成$X \in R^{C×T}$，这里采用二维卷积函数，而不是一维卷积函数。
::: tip EEGNet中对卷积核长度的解释
256Hz,选用128的长度可以捕获2Hz及以上的频率信息。
:::
多卷积层的主要优点包括:
- 从不同的维度对脑电信号进行紧凑、全面和完整的表征;
- 以最优方法探索提取的特征映射与特征融合之间的关系;
- 参数较少，适合分采样层的实现。
#### BN层
采用`Batch归一化`(BN)方法对每个训练小Batch进行**归一化处理**，通过减少内部协变量偏移来加快网络训练过程。
#### ELU层
在卷积层和反卷积层中加入**指数线性单元(ELU)激活函数**，改进模型拟合。


#### 功能
可以从不同大脑位置采集的时序脑电图信号中**学习时空动态**，并将样本点**集成到一个紧凑而深入的特征表示向量**，该向量已被证明有助于准确、高效的数据描述。该网络可为无监督深度特征表征和融合提供基础。训练过程中的**权重是随机初始化**的。

## 混合CNN-GAN
::: tip 为什么要使用GAN
传统的`编码器-解码器网络`易于训练，但生成的特征质量较低。许多研究已经证明**GAN**能够从顺序数据**生成高质量的特征**。
:::

::: tip 鉴别器?鉴别啥?
鉴别器是对输入的脑电信号是由**生成器产生的伪脑信号**以及从人脑中采集到的**真实脑信号**进行**区分**。
:::
::: tip 生成器?生成啥?
生成器生成**判别器无法判别的信号**, 但是生成器的目的就像`枯叶蝶`一样,他并不是要变成一片树叶,而是要骗过捕食者,也就是判别器
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220912170951.png)
:::
为了进一步了解非平稳时序脑电图数据的复杂结构，我们进一步开发了一种**基于CNN-GAN的混合深度编码器-解码器网络**。
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220912185825.png)

|符号|含义|
|--|--|
|$G$|生成器|
|$D$|鉴别器|

### 目标函数
训练过程中的**目标函数**是建立一个好的`D`，能够从生成的假样本中区分出真实的样本，同时开发一个好的`G`，能够产生一个尽可能与真实样本相似的假样本`双人极大极小博弈`。在训练过程中，判别器的输入是X和G(X)。目标函数为
$$L_{Gan} ( G , D ) = E_X [ \log D ( X ) ] + E_x \log [ 1 - D ( G ( X ) ) ]$$
$$L _ { 1 } ( G ) = | | X - G ( X ) ) | | _ { 2 } ^ { 2 }$$

|符号|含义|
|--|--|
|$\log D ( X )$|真实样本X的鉴别器输出|
|$D(G(X))$|假样本的鉴别器输出|

混合CNN-GAN网络的总体目标函数为
$$L = \operatorname { a r g}{ m i n } _ { G } max_D ( L _ { G A N } ( G _ { A } D ) + \lambda L _ { 1 } ( G ) )$$
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220912201415.png)
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220912195917.png)
::: warning 汇报任务:
- 解释L1
- 指出原文与GAN描述不同的地方
:::

## 混合CNN-RNN-GAN (EEGFuseNet)
最终模型提取的向量作为深度脑电特征，用于后续的无监督脑电解码，可以跨时间点表示整个输入脑电信号，不仅包括脑电特征，还包括**序列信息**中的脑电特征。该结构成功地融合了提取的**不同level**、**不同脑位置**、**不同时间点**的特征表示，有利于非平稳时间序列脑电图信号的时空动态表征。
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220912202522.png)
### 结构
- **卷积层**在每个时间点对脑电图信号进行特征提取(`浅特征提取`)
- **循环层**将每个时间点提取的特征编码为整个输入脑电图信号的整个特征表示(`深特征提取`)
- 解码器由**循环层和反卷积层**组成，从编码器输出信号预测每个时间点的特征，并将特征重构为原始脑电信号。

::: tip 文章如何提取所谓的多维EEG信号

在`CNN`浅层特征提取部分，生成的特征图(行和列是来自**通道和时间点**的特征)在深层特征提取部分对序列特征进行表征 ，`LSTM单元`学习**相邻空间依赖关系**。
- 被表征为每个时间点t的一组特征向量序列(被认为是**空间动态表征**)

- 由循环层学习序列特征，合成时间序列脑电信号的过去和未来动态信息(被认为是**时间动态表征**)。


:::
~~我们提出了~~一种类似于`LSTM`的`GRU`, 为了获得更高的计算效率，我们在实现中采用了**双向GRU**，定义为:
$$ \begin{aligned} z_{t} &=\sigma\left(W^{(z)} x_{t}+U^{(z)} h_{t-1}+b^{(z)}\right) \\ r_{t} &=\sigma\left(W^{(r)} x_{t}+U^{(r)} h_{t-1}+b^{(r)}\right)\\ \hat{h}_{t} &=\phi\left(W^{(h)} x_{t}+U^{(h)}\left(r_{t} \odot h_{t-1}\right)+b^{(h)}\right) \end{aligned} $$
$$ h_{t}=\left(1-z_{t}\right) \odot h_{t-1}+z_{t} \odot \hat{h}_{t} $$
::: tip 什么是GRU?
是`LSTM`的一种变体, 它可以在不分离记忆单元的情况下调节门控单元内的提示流。很明显，在机器学习任务上，`GRU`表现出了与`LSTM`相当的性能，需要的参数更少一些。
:::

|符号|含义|
|--|--|
|$x_t (t \in [1,t])$|输入|
|$h_t (t \in [1,t])$|输出|
|$T$|是数据的总长度|
|$W^{(z)}， W^{(r)},W^{(h)}， U^{(z)}， U^{(r)}，U^{(h)}$|是权重矩阵|
|$b^{(z)}， b^{(r)}， b^{(h)}$|是在训练过程中学习到的偏差|
|$Z_t$|更新门向量|
|$r_t$|复位门向量|
|$\hat h^t$|隐态向量|
|$σ$|sigmoid函数|
|$φ$|tangent函数|
|$\odot$|元素级的乘法|

### 计算部分
在实现过程中，前向和后向循环层对序列中**基于时间点的特征向量**进行**迭代运算**，并计算出相应的**隐藏状态向量**的前向和后向序列。

- 数据$[{x_1, x_2，…， x_T}]$分别按**前向和后向顺序**输入`到双向GRU`。这里，`前向和后向GRU`的隐藏层记为$[h^f_1, h^f_2，…h^f_T]$和$[h^b_1, h^b_2，… ,h^b_T]$。`前向和后向GRU`在t时刻的输出分别为
$$ \begin{aligned} h_{t}^{f} &=G R U\left(x_{t}, h_{t-1}^{f}\right) \\ h_{t}^{b} &=G R U\left(x_{t}, h_{t-1}^{b}\right) \end{aligned} $$
`双向GRU`在t点的输出为$a_{t}=h_{t}^{f} \oplus h_{t}^{b}$，其中⊕表示**向量拼接**。

- 生成的深度特征表示向量$o$表示为$o = (a_1，…a_t…， a_T)$，获取序列特征信息。

### 训练过程
为了提高实现效率，我们在训练过程中**分批次更新GRU的输入**，其中一批包括在一定时间间隔内连续的脑电信号。



## 基于无监督的超图解码模型


::: tip 超图简介
与简单图不同，超图能够连接共享相似属性的两个顶点（多于两个），呈现更一般类型的关系，并显示比单个连接更复杂的隐藏结构。
:::
情感分类是通过计算EEG样本的相似性分布形成的超边以连接多个EEG样本作为分类依据。
- 计算 **hypergraph Laplacian** 
- 用**最优特征空间**求解
- 将构造的超图**划分**为特定数量的类, 每个类表示一种情绪状态。

## 训练
|参数|大小/方法|
|--|--|
|卷积层中的权重参数初始化|基于Glorot初始化采用均匀分布进行初始化|
|epoch|100|
|优化器|Adam优化器+随机梯度下降（SGD）方法|
|动量|0.9|
|生成器学习率|0.001|
|鉴别器学习率|0.0002|
|batch大小|128|
|硬件平台|NVIDIA GeForce RTX 2080|

## 结果
### DEAP数据集不同维度分类水平
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220912234314.png#pic_center%20=400x)


为了对我们提出的方法和`Liang等`人的工作进行公平比较，我们使用了一个受试者不受交叉验证的受试者独立协议（与我们提出方法中使用的协议相同）评估了他们的工作。比较结果表明，我们提出的算法优于`Liang等`
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913135116.png#pic_center%20=400x)

```js
// --echarts--
function (chart) {
chart.setOption({
    // backgroundColor: '#2c343c',

    title: {
        text: 'DEAP数据集F1-Sore比较结果',
        left: 'center',
        top: 0,
        textStyle: {
            color: '#888'
        }
    },
    legend: {top: 30,},
    tooltip : {
        trigger: 'axis',
        axisPointer: {
        type: 'shadow'
        }
    },
    toolbox: {
        show: true,
        orient: 'vertical',
        left: 'right',
        top: 'center',
        feature: {
        mark: { show: true },
        dataView: { show: true, readOnly: false },
        magicType: { show: true, type: ['line', 'bar', 'stack'] },
        restore: { show: true },
        saveAsImage: { show: true }
        }
  },
    dataset: {
          // 提供一份数据。
          source: [
              ['MSE', 'valence', 'arousal','dominance','liking'],
              ['Liang', 53.45,52.77,52.84,59.99],
              ['Proposed Method', 70.83, 72,74.32,78.46],
          ]
      },
    xAxis: {type: 'category'},
    yAxis: {type: 'value'},
    series: [
        {
            type: 'bar',
            emphasis: {
                focus: 'series'
            },
            itemStyle: {
              normal: {
                label: {
                  show: true, //开启显示
                  position: 'top', //在上方显示
                  textStyle: {
                    //数值样式
                    color: 'black',
                    fontSize: 12,
                  },
                },
              },
            }, 
          },
        {
            type: 'bar',
            emphasis: {
                focus: 'series'
            },
            itemStyle: {
              normal: {
                label: {
                  show: true, //开启显示
                  position: 'top', //在上方显示
                  textStyle: {
                    //数值样式
                    color: 'black',
                    fontSize: 12,
                  },
                },
              },
            }, 
          },
        {
            type: 'bar',
            emphasis: {
                focus: 'series'
            },
            itemStyle: {
              normal: {
                label: {
                  show: true, //开启显示
                  position: 'top', //在上方显示
                  textStyle: {
                    //数值样式
                    color: 'black',
                    fontSize: 12,
                  },
                },
              },
            }, 
          },
        {
            type: 'bar',
            emphasis: {
                focus: 'series'
            },
            itemStyle: {
              normal: {
                label: {
                  show: true, //开启显示
                  position: 'top', //在上方显示
                  textStyle: {
                    //数值样式
                    color: 'black',
                    fontSize: 12,
                  },
                },
              },
            }, 
          }, 
      ]
}, true)
}
```
```js
// --echarts--
function (chart) {
chart.setOption({
    // backgroundColor: '#2c343c',

    title: {
        text: 'DEAP数据集Accuracy比较结果',
        left: 'center',
        top: 0,
        textStyle: {
            color: '#888'
        }
    },
    legend: {top: 30,},
    tooltip : {
        trigger: 'axis',
        axisPointer: {
        type: 'shadow'
        }
    },
    toolbox: {
        show: true,
        orient: 'vertical',
        left: 'right',
        top: 'center',
        feature: {
        mark: { show: true },
        dataView: { show: true, readOnly: false },
        magicType: { show: true, type: ['line', 'bar', 'stack'] },
        restore: { show: true },
        saveAsImage: { show: true }
        }
  },
    dataset: {
          // 提供一份数据。
          source: [
              ['MSE', 'valence', 'arousal','dominance','liking'],
              ['Liang', 54.30,55.55,57.03,58.91],
              ['Proposed Method', 56.44, 58.55,61.71,65.89],
          ]
      },
    xAxis: {type: 'category'},
    yAxis: {type: 'value'},
    series: [
        {
            type: 'bar',
            emphasis: {
                focus: 'series'
            },
            itemStyle: {
              normal: {
                label: {
                  show: true, //开启显示
                  position: 'top', //在上方显示
                  textStyle: {
                    //数值样式
                    color: 'black',
                    fontSize: 12,
                  },
                },
              },
            }, 
          },
        {
            type: 'bar',
            emphasis: {
                focus: 'series'
            },
            itemStyle: {
              normal: {
                label: {
                  show: true, //开启显示
                  position: 'top', //在上方显示
                  textStyle: {
                    //数值样式
                    color: 'black',
                    fontSize: 12,
                  },
                },
              },
            }, 
          },
        {
            type: 'bar',
            emphasis: {
                focus: 'series'
            },
            itemStyle: {
              normal: {
                label: {
                  show: true, //开启显示
                  position: 'top', //在上方显示
                  textStyle: {
                    //数值样式
                    color: 'black',
                    fontSize: 12,
                  },
                },
              },
            }, 
          },
        {
            type: 'bar',
            emphasis: {
                focus: 'series'
            },
            itemStyle: {
              normal: {
                label: {
                  show: true, //开启显示
                  position: 'top', //在上方显示
                  textStyle: {
                    //数值样式
                    color: 'black',
                    fontSize: 12,
                  },
                },
              },
            }, 
          }, 
      ]
}, true)
}
```


$P_{acc}$和${P_f}$的平均增长率分别为**7.35%**和**35.10%**。结果还表明，与使用的手工特征相比，`EEGFuseNet`的特征化深度特征可能对个体差异不太敏感。
### 特征表征能力比较
我们将EEGFuseNet的特征表征能力与文献中常用的传统EEG特征进行了比较，如时域特征、功率谱特征和差分熵特征。时域特征表征了时间序列EEG数据的统计模式、Hjorth特征和形状信息；功率谱特征表征不同频带的频谱功率；差分熵特征表征不同频带的差分熵。为了使结果具有可比性，将提取的传统特征和EEGFuseNet特征映射到相同的特征维度，并输入超图解码模型以实现情感识别。超图解码模型中的参数与所提出的方法相同，其中κ和η值分别设置为5和10。
除了$P_{acc}$和${P_f}$之外，我们还引入了归一化互信息（NMI）作为另一种性能度量来比较评估性能并检查相应的聚类质量。值得注意的是，所有模型性能评估都是使用`leave-one`受试者交叉验证受试者独立评估协议进行的。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220912234748.png#pic_center%20=400x)



### 消融研究
- CNN的编码器-解码器结构
- CNN的编码器-解码器结构+GAN模块
- CNN的编码器-解码器结构+RNN模块。

![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220912235142.png#pic_center%20=400x)
结果表明，EEGFuseNet优于这些变体模型，它更能够高质量地表征和融合与情绪相关的深层脑电特征，并实现更好的基于跨学科的情绪识别性能。此外，还演示了混合方法的好处。混合网络（基于CNNGAN、基于CNN-RNN和基于CNN-RNN-GAN）优于单一模型（基于CNN）。这些结果表明，混合网络在处理数据多样性问题方面更加灵活和稳定，更有利于高质量的EEG特征表征和跨时空动态的信息融合。

### 超参数 
#### 输入大小对结果和效率的影响
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220912235406.png#pic_center%20=400x)
第一卷积层和深度可分离卷积层中的核大小将根据输入数据大小自适应地调整。结果表明，32×384的相对较小的输入大小表现最佳，这显然覆盖了收集数据中的几乎所有重要信息，并提高了计算效率。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220912235513.png#pic_center%20=400x)
EEGFuseNet训练时间受输入大小影响不大，但EEGFusenet测试和超图解码的计算时间随着输入大小的增加而增加。

#### 超边缘大小对结果的影响
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913005014.png#pic_center%20=400x)

对于超图解码模型，还研究了超边缘大小（κ）对情感识别性能的影响。我们将κ值从5调整到35，步长为5，并在图中给出相应的情绪识别性能。结果表明，性能相对稳定，对κ值的变化不太敏感。
#### 超参数η对结果以及计算成本的影响
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913005024.png#pic_center%20=400x)

我们引入了一个超参数η%的加速定理，以减少计算量,这表明，η值的增加通常会导致更高的情绪识别准确率。对于η=2获得比η=3更好的性能的情况，η=1的随机选择的训练数据可能与测试数据共享相似的模式，并且涉及较少的个体差异。

![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913000311.png#pic_center%20=400x)
不同η值下的平均计算时间如图所示。η值的增加导致计算时间的指数增长，其中对于η=1，超图解码的成本时间为1496s，对于η=15，成本时间为62466s。对于η值为10，计算时间为11718s。解码性能和计算时间之间存在权衡。
## 补充
### 什么是超图
简单来说，我们所熟悉的图而言，它的一条边（edge）只能连接两个顶点（vertice）；而超图，人们定义它的一条边（hyperedge）可以和任意个数的顶点连接。图与超图的对比示意图如下：
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220911082529.png)
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220911082547.png)
在超图里，我们抽象出一种新的边：==超边==（hyperedge）。超边可以做到同时连接>2个点，如下图所示：
![Img](https://imgpool.protodrive.xyz/IMG/yank-note-picgo-img-20220911112942.png)

|符号|含义|
|--|--|
|$v_1,v_2...v_7$|代表七个节点|
|$e_1,e_2,e_3$|代表了三条超边(hyperedge)也就是图中的三个椭圆|

以`KNN超图`举例,假设图中每个节点都是一个菌种，每个节点$v_i$的赋值内容则是一个包含有菌种各种信息的向量。
```mermaid
graph TD
F[选择一个新节点] --> A[通过计算比较各个菌种信息的Euclidean distance]
A --> D[选择k个neighbour]
D --> B[据此构建出一个包含k+1个超边,原节点也包含在内的]
B -->  C{是否所有节点都计算完毕?}
C --> |yes| E[构建出一张完整的超图]
C --> |No|F
```
总的来说，**超图的建立是基于超边的建立，而超边的建立则是基于我们对节点之间关系的先验条件来实现的**。
### 超图的拉普拉斯矩阵
作为普通图的拉普拉斯矩阵的拓展，超图拉普拉斯矩阵与普通图的拉普拉斯矩阵有许多向通的地方。从形式上来说，超图的拉普拉斯矩阵同样也能由 L = 度矩阵-邻接矩阵 表示，且度矩阵与普通图的拉普拉斯矩阵公式一致，但在邻接矩阵的定义上稍有不同
$L=D-W$
$W$为邻接矩阵，$D$为节点的度（degree）矩阵

超图的邻接矩阵公式定义如下：
$$ \Theta=H * W * D_{e}^{-1} * H^{T} $$
H为超图的点边关联矩阵，$D_{e}$是超边的度矩阵。
下面还是先从构建一个超图开始
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913080623.png#pic_center%20=400x)
可得点边关系矩阵$H$
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913080645.png#pic_center%20=400x)
$D_{e}$超边的度矩阵
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913080706.png#pic_center%20=400x)
各个超边权值暂且设为$w_1,w_2,w_3$  ，则有超边权值矩阵$W$
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913080757.png#pic_center%20=400x)
节点的度矩阵$D_{v}$
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913080819.png#pic_center%20=400x)
由上述矩阵计算得到该超图的拉普拉斯矩阵 $L$
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220913080836.png#pic_center%20=400x)
