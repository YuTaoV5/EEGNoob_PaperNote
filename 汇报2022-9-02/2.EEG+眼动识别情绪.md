---
headingNumber: true
enableMacro: true
customVar: Hello
define:
    --Author--: ProtoDrive000
    --te--: ==transformer encoders==
    --sa--: ==self-attention==
---
# 2.EEG+眼动识别情绪


| 论文名称 |Emotion Transformer Fusion: Complementary Representation Properties of EEG and Eye Movements on Recognizing Anger and Surprise|
| -- | -- | 
| 期刊 |IEEE International Conference on Bioinformatics and Biomedicine (BIBM)|
| 方法 |我们建立了一个新的多模态情绪数据集，包括愤怒、惊讶和中性三种情绪的EEG和眼动信号，并构建了基于`Transformer`的多模态网络。因此，我们提出了情感变换融合作为一种纯粹的基于注意力的模型，将Transformer encoders与基于注意力的融合相结合，以利用EEG和眼动信号在情感识别中的并行性和简单性。|
| 结论 |本文实施了多模态实验，成功地在不同类型的刺激下激发愤怒、惊讶和中性情绪，这也说明EEG和眼动信号对识别这三种情绪是互补的。此外，基于纯注意机制的情感变换融合被用于多模态情感识别。所提出的多模态模型的最佳准确率为90.02%，标准偏差为7.06%，证实了基于`Transformer`的架构和基于注意力的融合在多模态情感识别上有效工作。单模态的识别结果也表明了在EEG或眼动中使用`Transformer`的有效性。|
| 评价 |我认为Transformer必然是提取特征的最重要方法,以CLIP举例,我认为多模态也应该具有远大的前途|
---
::: tip 为什么选择分类愤怒和惊讶情绪?
在六种基本情绪中，愤怒和惊讶在实验室环境中相对较难被激发出来，脑图(EEG)和眼动信号在识别愤怒和惊讶情绪方面的互补表征特性仍然未知。
:::
据我们所知，尽管Transformer已成为自然语言处理(NLP)和计算机视觉(CV)任务中最具影响的架构，但其在EEG和眼动信号的==多模态==情感识别中的应用仍然有限。

## EEG信号获取
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220903032156.png#pic_center%20=400x)

每个阶段设计了11个试验。尽管我们的目标是调查愤怒和惊讶，但我们在实验和最终分类中添加了**中性情绪**，这调解了受试者的两种极端情绪，并促进了更好的诱导。每个试验包含两个部分：**情绪诱导**和**自我评估**，让受试者将情绪唤醒水平从1分到10分进行评价。

前六次试验旨在通过视频剪辑交替触发惊喜和中性情绪。在实验室环境中，不同的受试者很难唤起愤怒，持续刺激可能会产生更好的效果。以这种方式，连续安排了五次试验

- 第一次试验作为促进部分，旨在将受试者的情绪状态迅速转变为愤怒。

- 然后是观看关于社会不公正的视频剪辑。经过三次常规的视频试验后，会有一个特殊的回忆片段要求受试者记住最近发生的让他们最愤怒的事件大约三分钟。

- 最后，一个精心设计的游戏由主体玩了大约20分钟。在实验过程中，除升压试验外，同时采集EEG和眼动信号。实验结束后，受试者接受采访，指出他们玩游戏和三次试验时最愤怒的时期，以保持每个class和试验的数据平衡。
## 眼动信号获取
对于眼动信号，我们使用`PCA`去除光反射，以提高瞳孔直径中的情感信息质量。我们提取了眼动信号的23个特征，其细节如图所示，其中块中的数字代表维度，蓝色的四个特征也是事件统计。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220903032403.png#pic_center%20=400x)
## 模型框架
其中灰色和蓝色的部分分别是EEG和眼睛运动的`Transformer`编码器。在==位置编码==之后，来自两个模态的序列被馈入相应的`Transformer`编码器，然后通过基于注意力的融合层融合到联合表示空间.
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220903032501.png#pic_center%20=400x)


::: tip 为什么不需要Decoder?
因为我们不需要序列作为输出，所以执行作为分类器的全连接前馈网络，而不是解码器。
:::
输入的EEG信号和眼动信号表示为:
$$ \begin{aligned} X_{e e g} &=\left(x_{e e g}^{1}, x_{e e g}^{2}, \ldots, x_{e e g}^{T}\right) \in R^{B \times T \times D_{e e g}} \\ & X_{e y e}=\left(x_{e y e}^{1}, x_{e y e}^{2}, \ldots, x_{e y e}^{T}\right) \in R^{B \times T \times D_{eye}}\end{aligned} $$
| 符号 | 含义 |
| -- | -- |
|B|表示批次大小|
|T|表示重叠窗口大小|
|$D_{eeg}$|表示EEG的特征维度|
|$D_{eye}$|表示眼球运动的特征维度|
|$O=(o_1，o_2，o_3)$|模型输出向量|
|$o_i$|表示情绪被识别为i类的概率|

## 信号段和编码
由于EEG和眼动信号(尤其是后者)的特征大小**相对较小**，因此我们在模型中不执行`dropout`。为了**规范化数据**、**缓解过度拟合**和**提高学习速度**，我们在开始时对小批量应用`Batch Normalization`。

考虑到EEG和眼球运动的时间序列太长，无法直接输入网络，我们部署了重叠窗口的大小为T秒，这将保持总样本大小(每个实验大约1200秒)不变。我们在工作中选择T作为5秒。

- 一个额外的可学习分类标记`class token`连接在序列的开头以执行分类。
- 然后将`位置嵌入`添加到`patch`元素中，以获得时间序列的信息。

## Encoder
L个相同的层堆叠以组装编码器，每个层由两个子层组成：`多头自注意力MSA`和`完全连接的前馈网络MLP`。每个子层都从`层归一化`开始，以消除内部协变量偏移。并且在每个子层周围存在剩余连接以保留输入特征的信息并增强模型稳定性。
::: tip GELU替代RELU的好处?
具有更好的性能，并避免了消失梯度问题
:::
- 首先,我们采用基于注意力的融合策略，因为我们的模型为基于纯注意力机制的。注意力权重$W_{init}$被随机初始化。

- 然后，我们使用两种模态的变换特征计算**注意力权重的内积**，然后使用`softmax`对结果进行归一化。在获得注意权重$w_{eeg}$和$w_{eye}$之后，使用的融合输出被计算为**单模态输出的加权和**。整个过程公式化如下：
$$ w_{e e g}^{\prime}=<O_{e e g}, W_{i n i t}> $$
$$ w_{e y e}^{\prime}=<O_{e y e}, W_{i n i t}> $$
$$ w_{e e g}, w_{e y e}=\operatorname{softmax}\left(w_{e e g}^{\prime}, w_{e y e}^{\prime}\right) $$
$$ O_{f u s e}=w_{e e g} O_{e e g}+w_{e y e} O_{e y e} $$

## 实施细节
### 实验设置
我们对数据集上的每个主题执行`三折交叉验证`。所有模型的性能通过所有实验的三折交叉验证精度进行评估。在我们的多模式架构中有四个超参数：
- `Transformer`编码器层L的**数量**
- 使用的融合输出的**维度**
- `Adam优化器`的**学习率**
- `Adam优化器`的**权重衰减**

## 结果分析和比较
### 单模态
单模态的基线模型是SVM、LSTM。ET表示情感变换器，它是所提出模型的单Transformer，没有基于注意力的融合。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220903034227.png#pic_center%20=400x)

我们将LSTM作为基线模型，因为LSTM及其变体广泛应用于情感识别，其利用时间特征的特性使其与Transformer具有可比性。
::: tip Transformer的优势
EEG在识别数据集的三种情绪方面显著优于眼动信号。提出的情感转换器超过了基线模型，准确率分别为83.3%和77.86%，标准偏差为10.97%和13.46%

由于我们保持`Transformer`编码器与原始编码器ViT尽可能相似，这表明`Transformer`识别情感的有效性。
:::
### 多模态
对于多模态，所有模型都基于联合表示，以减少性能评估的变量。基线模型包括
- 双峰深度自动编码器(BDAE)
- 具有与所提出模型相同融合策略的LSTM
- 用直接级联代替基于注意力的融合的ETConcat
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220903034240.png#pic_center%20=400x)
::: tip 多模态的优势
**多模态LSTM和Transformer都比每种单一模态的性能更好**，这证明组合模态可以显著提高情感识别性能。
::: 

::: tip 注意力的融合的优势
如表所示的最后两列，`ETConcat`的准确性和标准偏差比`ETF`差，这表明基于注意力的融合比直接级联能够从EEG和眼动信号中提取更多情感相关特征。
:::
::: tip 精度优势
我们提出的模型ETF达到了90.02%的最高精度和7.06%的最低标准差，表明了我们模型的有效性。
:::